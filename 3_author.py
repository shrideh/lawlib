import os
import json
import warnings
import logging
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# Ø¥Ø®ÙØ§Ø¡ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ÙˆØ§Ù„Ø±Ø³Ø§Ø¦Ù„ ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
warnings.simplefilter("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("transformers.modeling_utils").setLevel(logging.CRITICAL)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬
model_name = "E:/mws_server/import_pdf_py_file/models/models--CAMeL-Lab--bert-base-arabic-camelbert-mix-ner"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³Ù…Ø§Ø©
ner_pipeline = pipeline(
    "ner",
    model=model,
    tokenizer=tokenizer,
    aggregation_strategy="simple"
)

def load_json_file(file_path):
    """ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù JSON ÙˆØ¥ØµÙ„Ø§Ø­Ù‡ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¹Ø·ÙˆØ¨Ù‹Ø§ """
    try:
        with open(file_path, 'r', encoding='utf-8') as json_file:
            content = json_file.read().strip()
            if not content:
                raise ValueError("Empty file")
            data = json.loads(content)
    except (json.JSONDecodeError, ValueError):
        print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ø§Ù„Ù…Ù„Ù {file_path} Ù…Ø¹Ø·ÙˆØ¨ Ø£Ùˆ ÙØ§Ø±ØºØŒ Ø³ÙŠØªÙ… Ø¥ØµÙ„Ø§Ø­Ù‡!")
        data = {"contents": []}  # ØªØ¹ÙŠÙŠÙ† Ø¨ÙŠØ§Ù†Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        with open(file_path, 'w', encoding='utf-8') as json_file:
            json.dump(data, json_file, ensure_ascii=False, indent=4)
    return data

def extract_text_from_json(json_data, max_pages=200):
    """ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø£ÙˆÙ„ max_pages ØµÙØ­Ø§Øª Ù…Ù† JSON """
    extracted_text = " ".join(
        page.get("text", "") for page in json_data.get("contents", [])[:max_pages]
    )
    return extracted_text.strip()

def find_json_files(json_folder):
    """ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª JSON Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙˆØ§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ© """
    json_files = []
    for root, _, files in os.walk(json_folder):
        for filename in files:
            if filename.endswith('.json'):
                json_files.append(os.path.join(root, filename))
    return json_files

def split_text_into_chunks(text, tokenizer, max_length=512):
    """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² 512 ØªÙˆÙƒÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… tokenizer"""
    tokens = tokenizer.tokenize(text)
    chunks = []
    
    for i in range(0, len(tokens), max_length - 10):  # ØªØ±Ùƒ Ù…Ø³Ø§Ø­Ø© ØµØºÙŠØ±Ø© Ù„ØªØ¬Ù†Ø¨ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
        chunk_tokens = tokens[i:i + max_length - 10]
        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)
        chunks.append(chunk_text)
    
    return chunks

def extract_named_entities(text, max_entities=5, min_word_length=4):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ max_entities ÙƒÙŠØ§Ù†Ù‹Ø§ Ù…Ø³Ù…Ù‰ØŒ Ù…Ø¹ ØªØµÙÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ù‚Ù„ Ù…Ù† min_word_length"""
    if not text.strip():
        return []

    chunks = split_text_into_chunks(text, tokenizer, max_length=512)
    extracted_entities = []

    for chunk in chunks:
        entities = ner_pipeline(chunk[:512])  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰
        for entity in entities:
            word = entity["word"].strip()
            if len(word) >= min_word_length:
                extracted_entities.append({
                    "text": word,
                    "type": entity["entity_group"],
                    "score": round(float(entity["score"]), 3)
                })
            
            # Ø¥Ø°Ø§ ÙˆØµÙ„Øª Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ØŒ ØªÙˆÙ‚Ù
            if len(extracted_entities) >= max_entities:
                return extracted_entities

    return extracted_entities[:max_entities]

json_folder = 'E:\\mws_server\\import_pdf_py_file\\PDF_JSON'
files = find_json_files(json_folder)

for file in files:
    data = load_json_file(file)  # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª JSON (Ù…Ø¹ Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø¥Ù† Ù„Ø²Ù…)
    
    text = extract_text_from_json(data, max_pages=5)
    book_info = extract_named_entities(text, max_entities=5, min_word_length=4)

    if "book_info" in data:
        print(f"ğŸ“Œ ØªØ­Ø¯ÙŠØ« Ø¨ÙŠØ§Ù†Ø§Øª book_info ÙÙŠ {file}")
    else:
        print(f"âœ… Ø¥Ø¶Ø§ÙØ© book_info Ø¥Ù„Ù‰ {file}")

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø­ÙØ¸Ù‡Ø§
    data["book_info"] = book_info
    with open(file, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, ensure_ascii=False, indent=4)

print("\nğŸ¯ ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†Ø¬Ø§Ø­!")

